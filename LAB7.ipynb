{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Universidad del valle de Guatemala  \n",
        "Dpto. Ciencias de la computacion  \n",
        "Inteligencia Artificial  \n",
        "Alberto Suriano  \n",
        "\n",
        "Laboratorio 7  \n",
        "Andres Quinto - 18288  \n",
        "Marlon Hernández - 15177  \n",
        "\n",
        "[LINK_REPO](https://github.com/AndresQuinto5/IA_LAB7.git)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1 - Teoria\n",
        "\n",
        "1. **¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado?** Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo  \n",
        "\n",
        "\n",
        "**El Temporal Difference Learning (TD Learning)** es una técnica de aprendizaje por refuerzo que no requiere un modelo del entorno y que puede aprender directamente de la experiencia cruda. A diferencia del aprendizaje supervisado, que se basa en un conjunto de datos etiquetados para entrenar algoritmos que clasifican datos o predicen resultados con precisión, el TD Learning opera a través de un proceso de prueba y error, ajustando sus predicciones basadas en la diferencia entre las recompensas esperadas y las recompensas reales obtenidas.\n",
        "\n",
        "El error de diferencia temporal es la diferencia entre la recompensa predicha en un estado y la recompensa real más la predicción del siguiente estado. Este error se utiliza para actualizar las predicciones de valor de un estado, permitiendo que el agente aprenda políticas óptimas a través de su experiencia. En el contexto del aprendizaje por refuerzo, este método permite al agente ajustar sus expectativas de manera incremental y aprender qué acciones conducen a mejores resultados a largo plazo.\n",
        "\n",
        "[ScholarPedia TD_learning](http://www.scholarpedia.org/article/Temporal_difference_learning)\n",
        "\n",
        "2. **En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes?** De un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación  \n",
        "\n",
        "En los juegos simultáneos, los jugadores toman decisiones sin conocer las acciones de los otros participantes. Esto se debe a que todos los jugadores eligen sus estrategias al mismo tiempo. Un ejemplo clásico de un juego simultáneo es el **Dilema del Prisionero**, donde dos cómplices son arrestados y deben decidir independientemente si confiesan o no. El resultado de sus decisiones depende de la elección del otro, pero ninguno sabe qué decidirá su compañero. \n",
        "\n",
        "[JuegoSimultaneo](https://policonomics.com/es/juego-simultaneo/)\n",
        "\n",
        "Un escenario del mundo real que puede modelarse como un juego simultáneo es el mercado de las telecomunicaciones (duopolio de Guatemala), donde varias empresas deciden al mismo tiempo si bajan los precios de sus servicios. Cada empresa debe elegir su estrategia sin saber qué harán las demás. Si una empresa baja sus precios y las demás no, podría ganar más clientes. Pero si todas bajan los precios, todas podrían terminar ganando menos.\n",
        "\n",
        "Las estrategias que los jugadores podrían emplear en tales situaciones incluyen:\n",
        "\n",
        "**Estrategia Dominante88:** Elegir la acción que proporciona el mejor resultado posible, sin importar lo que hagan los demás.\n",
        "**Equilibrio de Nash:** Una situación donde ningún jugador se beneficiaría cambiando su estrategia mientras los demás mantienen la suya.\n",
        "**Estrategias Mixtas:** Usar una estrategia probabilística, eligiendo diferentes acciones con ciertas probabilidades para maximizar la utilidad esperada.\n",
        "\n",
        "[Teoria de Juegos simultaneos](https://policonomics.com/es/lp-teoria-juegos2-juego-simultaneo/)\n",
        "\n",
        "3. **¿Qué distingue los juegos de suma cero de los juegos de suma cero y cómo afecta esta diferencia al proceso de toma de decisiones de los jugadores?** Proporcione al menos un ejemplo de juegos que entren en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas  \n",
        "\n",
        "Los juegos de **suma cero** son aquellos en los que las ganancias de un jugador se equilibran exactamente con las pérdidas de los otros jugadores. En estos juegos, el beneficio total es constante; lo que un jugador gana es lo que otro jugador pierde. Esto lleva a una competencia directa, ya que el éxito de un jugador significa necesariamente el fracaso de otro.\n",
        "\n",
        "Por otro lado, los juegos de **no suma cero** permiten que todos los jugadores ganen o pierdan simultáneamente. Estos juegos reflejan situaciones más realistas donde la cooperación y la competencia coexisten. Las decisiones no solo se basan en maximizar las propias ganancias a expensas de los demás, sino también en estrategias que pueden beneficiar a todos los jugadores.\n",
        "\n",
        "Un ejemplo clásico de un juego de **no suma cero** es el **Dilema del Prisionero**. En este juego, dos cómplices están detenidos y cada uno tiene la opción de cooperar con su compañero permaneciendo en silencio o traicionarlo confesando. Si ambos cooperan, reciben una pena leve; si uno traiciona y el otro coopera, el traidor queda libre mientras que el cooperador recibe una pena severa; si ambos traicionan, ambos reciben una pena moderada.\n",
        "\n",
        "Las consideraciones estratégicas en juegos de **no suma cero** incluyen la posibilidad de formar alianzas y la importancia de predecir y responder a las estrategias de los otros jugadores. Los jugadores deben considerar no solo sus propias recompensas sino también las recompensas de los demás, ya que las acciones de un jugador pueden afectar el resultado de todos.\n",
        "\n",
        "[Juego suma cero vs NO suma cero](https://www.rankia.co/blog/analisis-colcap/4595268-que-juego-suma-cero-no)\n",
        "[Dilema del prisionero](https://es.wikipedia.org/wiki/Dilema_del_prisionero) \n",
        "\n",
        "4. **¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos?** Explicar cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida  \n",
        "\n",
        "El concepto de **equilibrio de Nash** se aplica a los juegos simultáneos de una manera que refleja una situación en la que cada jugador, al no conocer las estrategias de los demás, elige una estrategia que considera óptima dado lo que espera que sean las estrategias de los otros jugadores. En un equilibrio de Nash, ningún jugador tiene un incentivo para cambiar su estrategia unilateralmente porque hacerlo no mejoraría su resultado. [Teoria de Juegos simultaneos](https://policonomics.com/es/lp-teoria-juegos2-juego-simultaneo/)\n",
        "\n",
        "En términos más técnicos, un conjunto de estrategias constituye un equilibrio de **Nash** si, dada la estrategia de cada uno de los otros jugadores, ningún jugador puede obtener un mayor pago cambiando su propia estrategia. Esto representa una solución estable porque cada jugador asume que los demás están haciendo lo mejor que pueden dada la situación, y por lo tanto, cualquier desviación solo reduciría su propio bienestar. [Equilibrio de Nash](https://es.wikipedia.org/wiki/Equilibrio_de_Nash)\n",
        "\n",
        "Por ejemplo, consideremos un mercado donde dos empresas deciden simultáneamente la cantidad de producto que van a producir. Si ambas empresas eligen la cantidad de producción que maximiza su ganancia esperada, dadas las cantidades producidas por la competencia, han alcanzado un equilibrio de Nash. Ninguna de las dos tendría un incentivo para cambiar su nivel de producción porque asumen que la otra empresa está produciendo la cantidad óptima para sí misma. https://es.wikipedia.org/wiki/Equilibrio_de_Nash\n",
        "\n",
        "Este tema es muy interesante y lo profundizamos mas en la clase de microeconomia (los de videojuegos).\n",
        "\n",
        "5. **Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos.** ¿Cómo maneja el temporal difference learning el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?  \n",
        "\n",
        "**El Temporal Difference Learning (TD Learning)** es especialmente útil en entornos dinámicos donde los procesos de toma de decisiones deben adaptarse continuamente a condiciones cambiantes. En el modelado y optimización de estos procesos, TD Learning permite que un agente aprenda una política de decisiones que maximiza la recompensa a largo plazo, ajustando sus predicciones basadas en la diferencia entre las recompensas esperadas y las obtenidas.\n",
        "[Temporal Difference Learning](http://www.scholarpedia.org/article/Temporal_difference_learning)\n",
        "\n",
        "El equilibrio entre exploración (probar nuevas acciones para descubrir recompensas potenciales) y explotación (usar el conocimiento actual para maximizar la recompensa) es crucial en TD Learning. Los algoritmos de TD suelen incorporar mecanismos como la política ε-greedy, donde con cierta probabilidad ε se elige una acción al azar (exploración), y con probabilidad 1-ε se elige la acción que maximiza la recompensa esperada (explotación). (Bastante similar a los temas que vimos en semanas anteriores). \n",
        "[Standford_EDU](https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/11-12-TD.pdf).\n",
        "\n",
        "Entre los desafíos de implementar TD Learning en la práctica se encuentran:\n",
        "\n",
        "- Representación del Estado: Diseñar una representación adecuada del estado que capture toda la información relevante para la toma de decisiones.\n",
        "- Ajuste de Parámetros: Calibrar parámetros como la tasa de aprendizaje y el factor de descuento para equilibrar la velocidad de aprendizaje y la estabilidad.\n",
        "- Generalización: Utilizar aproximación de funciones para generalizar a partir de experiencias previas a situaciones no vistas.\n",
        "- Escalabilidad: Manejar la maldición de la dimensionalidad cuando el espacio de estados o acciones es muy grande.\n",
        "[Practical issues in temporal difference learning](https://link.springer.com/article/10.1007/BF00992697)\n",
        "[Practical issues in temporal difference learning 2](https://dl.acm.org/doi/10.5555/2986916.2986948)\n",
        "\n",
        "*SARSA O Q-Learning?*\n",
        "\n",
        "**Q-learning** es un algoritmo de aprendizaje por refuerzo off-policy, lo que significa que aprende la política óptima independientemente de la política seguida por el agente mientras explora el entorno. Esto puede ser ventajoso si deseas que tu IA de TD aprenda la mejor estrategia posible sin limitarse a las estrategias específicas utilizadas durante el entrenamiento. Q-learning tiende a ser más adecuado para situaciones donde la exploración del espacio de estados es más importante que la adherencia a una política específica.\n",
        "\n",
        "Por otro lado, **SARSA** es un algoritmo on-policy, lo que significa que aprende la política de valor basada en las acciones que realmente toma, incluyendo la exploración. Esto puede resultar en una política más segura y conservadora, ya que tiene en cuenta las posibles penalizaciones de las acciones de exploración. SARSA podría ser preferible si te interesa que la IA se comporte de manera óptima en relación con su política de exploración actual.\n",
        "\n",
        "[When to choose SARSA vs. Q Learning](https://stats.stackexchange.com/questions/326788/when-to-choose-sarsa-vs-q-learning)\n",
        "\n",
        "**Elegimos Q-learning** debido a que Connect Four es un entorno relativamente estable y predecible, y si nuestro objetivo es desarrollar una IA que maximice su rendimiento (es decir, ganar el juego), Q-learning podría ser más adecuado. Esto se debe a que Q-learning buscará aprender la política óptima sin estar restringido por la política de exploración del agente, lo que podría ser crucial para superar a los agentes que utilizan MiniMax y alpha-beta pruning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "0.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
